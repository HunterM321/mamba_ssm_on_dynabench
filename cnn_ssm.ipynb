{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynabench.dataset import DynabenchIterator, download_equation\n",
    "from models.mamba_library import MambaTower\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple 2D CNN model for grid data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Number of input channels.\n",
    "        output_size : int\n",
    "            Number of output channels.\n",
    "        hidden_layers : int\n",
    "            Number of hidden layers. Default is 1.\n",
    "        hidden_channels : int\n",
    "            Number of channels in each hidden layer. Default is 64.\n",
    "        padding : int | str | Tuple[int]\n",
    "            Padding size. If 'same', padding is calculated to keep the input size the same as the output size. Default is 'same'.\n",
    "        padding_mode : str\n",
    "            What value to pad with. Can be 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n",
    "        kernel_size : int\n",
    "            Size of the kernel. Default is 3.\n",
    "        activation : str\n",
    "            Activation function to use. Can be one of `torch.nn activation functions <https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity>`_. Default is 'relu'.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 scaling: str,\n",
    "                 hidden_layers: int = 1,\n",
    "                 hidden_channels: int = 64,\n",
    "                 padding: int | str | Tuple[int] = 'same',\n",
    "                 padding_mode: str = 'circular',\n",
    "                 kernel_size: int = 3,\n",
    "                 activation: str = 'ReLU'):\n",
    "        super().__init__()\n",
    "        self.scaling = scaling\n",
    "        self.input_layer = nn.Conv2d(input_size, hidden_channels, kernel_size, padding=padding, padding_mode=padding_mode)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Conv2d(hidden_channels, hidden_channels, kernel_size, padding=padding, padding_mode=padding_mode) for _ in range(hidden_layers)])\n",
    "        self.output_layer = nn.Conv2d(hidden_channels, output_size, kernel_size, padding=padding, padding_mode=padding_mode)\n",
    "\n",
    "        self.activation = getattr(nn, activation)()\n",
    "\n",
    "        # For downscaling and upscaling\n",
    "        if self.scaling == 'down':\n",
    "            self.scale_conv = nn.Conv2d(output_size, output_size, kernel_size=3, stride=3, padding=0)\n",
    "        else:\n",
    "            self.scale_conv = nn.ConvTranspose2d(output_size, output_size, kernel_size=3, stride=3, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "            Forward pass of the model. Should not be called directly, instead call the model instance.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            x : torch.Tensor\n",
    "                Input tensor of shape (batch_size, input_size, height, width).\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor of shape (batch_size, output_size, height, width).\n",
    "        \"\"\"\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        x = self.scale_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaCNNMOL(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 hidden_layers,\n",
    "                 hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN to downsample the input into lower spatial dimensions\n",
    "        self.in_cnn = CNN(input_size=input_size,\n",
    "                          output_size=input_size,\n",
    "                          scaling='down',\n",
    "                          hidden_layers=hidden_layers,\n",
    "                          hidden_channels=hidden_channels)\n",
    "        \n",
    "        # CNN to upsample the SSMed input into the original spatial dimensions\n",
    "        self.out_cnn = CNN(input_size=input_size,\n",
    "                           output_size=output_size,\n",
    "                           scaling='up',\n",
    "                           hidden_layers=hidden_layers,\n",
    "                           hidden_channels=hidden_channels)\n",
    "        \n",
    "        self.mamba = MambaTower(d_model=25,\n",
    "                                n_layers=3,\n",
    "                                ssm_layer='mamba')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, H, W = x.shape\n",
    "        \n",
    "        x = self.in_cnn(x)\n",
    "        x = rearrange(x, 'b c h w -> b c (h w)')\n",
    "        x = self.mamba(x)\n",
    "        x = rearrange(x, 'b c (h w) -> b c h w', h=5, w=5)\n",
    "        x = self.out_cnn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 5\n",
    "rollout = 1\n",
    "\n",
    "advection_train_iterator = DynabenchIterator(split=\"train\",\n",
    "                                           equation='advection',\n",
    "                                           structure='grid',\n",
    "                                           resolution='low',\n",
    "                                           lookback=lookback,\n",
    "                                           rollout=rollout)\n",
    "\n",
    "train_loader = DataLoader(advection_train_iterator, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print('Using device:', device)\n",
    "\n",
    "model = MambaCNNMOL(input_size=lookback,\n",
    "                 output_size=rollout,\n",
    "                 hidden_layers=3,\n",
    "                 hidden_channels=10).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    # Use tqdm for the outer loop to show epoch progress\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{10}\", unit=\"batch\") as pbar:\n",
    "        for i, (x, y, p) in enumerate(train_loader):\n",
    "            # print(x.shape)\n",
    "            # print(y.shape)\n",
    "            x, y = x[:, :, 0].float().to(device), y[:, :, 0].float().to(device)\n",
    "            # print(x.shape)\n",
    "            # print(y.shape)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            # print(y_pred.shape)\n",
    "            # print('\\n')\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the progress bar with loss information\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advection_test_iterator = DynabenchIterator(split=\"test\",\n",
    "                                            equation='advection',\n",
    "                                            structure='grid',\n",
    "                                            resolution='low',\n",
    "                                            lookback=5,\n",
    "                                            rollout=10)\n",
    "\n",
    "test_loader = DataLoader(advection_test_iterator, batch_size=32, shuffle=False)\n",
    "\n",
    "loss_values = []\n",
    "with tqdm(total=len(test_loader), desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "    for i, (x, y, p) in enumerate(test_loader):\n",
    "        x, y = x[:, :, 0].float().to(device), y[:, :, 0].float().to(device)\n",
    "        # print(y.shape)\n",
    "        y_pred = model(x)\n",
    "        # print(y_pred.shape)\n",
    "        # print('\\n')\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        # Append the loss to the list\n",
    "        loss_values.append(loss.item())\n",
    "        \n",
    "        # Update the progress bar with loss information\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"Mean Loss: {sum(loss_values) / len(loss_values)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpen355",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
